{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ikmcXIWVt3K"
      },
      "source": [
        "#1. Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axxUzmcfVFYh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import shap\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
        "from scipy.spatial import cKDTree\n",
        "from pyproj import CRS, Transformer\n",
        "from pykrige.ok import OrdinaryKriging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXsCzFSPV8Oh"
      },
      "source": [
        "#2. Converting to Longitude Latitude\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRbcyCk0VoS5"
      },
      "outputs": [],
      "source": [
        "# Loading the data\n",
        "data = pd.read_csv('UT Comp_Seq.csv')\n",
        "\n",
        "#the following function uses the pyproj transformer function to take long lat and transform it to feet coordinate system\n",
        "def convert_lonlat_to_feet(longitude, latitude, target_crs):\n",
        "    \"\"\"\n",
        "    Converts longitude and latitude coordinates to feet coordinate system.\n",
        "\n",
        "    Args:\n",
        "        longitude (float or list): Longitude value(s) in decimal degrees.\n",
        "        latitude (float or list): Latitude value(s) in decimal degrees.\n",
        "        target_crs (str): Target coordinate reference system in feet.\n",
        "                           Example: 'epsg:2263' for NAD83 / New York Long Island (ftUS)\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing x and y coordinates in feet.\n",
        "    \"\"\"\n",
        "    source_crs = CRS(\"epsg:4326\")\n",
        "    target_crs = CRS(target_crs)\n",
        "    #main function\n",
        "    transformer = Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
        "    x, y = transformer.transform(longitude, latitude)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# Target CRS: NAD83 / New York Long Island (ftUS)\n",
        "target_crs = 'epsg:2263'\n",
        "\n",
        "data['X (feet)'], data['Y (feet)'] = zip(*data.apply(lambda row: convert_lonlat_to_feet(row['Longitude'], row['Latitude'], target_crs), axis=1))\n",
        "\n",
        "#zeros the data for convenience\n",
        "data['X (feet)'] = data['X (feet)'] - data['X (feet)'].min()\n",
        "data['Y (feet)'] = data['Y (feet)'] - data['Y (feet)'].min()\n",
        "\n",
        "#saves to csv, the process can be quite lengthy, this is to save progress\n",
        "data.to_csv('UT Comp_Seq_Feet.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxFxD-6lWfUY"
      },
      "source": [
        "#3. Neighbor Counting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "optuWrqTWlLj"
      },
      "outputs": [],
      "source": [
        "# Loading the data from cell 2\n",
        "data = pd.read_csv('UT Comp_Seq_Feet.csv')\n",
        "\n",
        "#TF formation is divided into these following formations based on depth, combining them enables a better model\n",
        "data['Formation'] = data['Formation'].replace(['TF2', 'TF1', 'TF3', 'UTFH', 'TFSH', 'MTFH', 'TF2.5', 'TF4'], 'TFH')\n",
        "#convert it to a datetime object\n",
        "data['Date Fracd'] = pd.to_datetime(data['Date Fracd'], format='%m/%d/%Y')\n",
        "coords = data[['X (feet)', 'Y (feet)']].values\n",
        "#uses cKDTree function, a spatial algorithm for faster calculations\n",
        "tree = cKDTree(coords)\n",
        "\n",
        "# Calculate neighbor well count with formation weighting\n",
        "data['Neighbors_Count_UpToDate'] = 0  # Initialize the new column\n",
        "\n",
        "for i in range(len(data)):\n",
        "    current_date = data.loc[i, 'Date Fracd']\n",
        "    neighbors_indices = tree.query_ball_point(coords[i], r=1000)\n",
        "    #Counts the wells around the datapoint at the time of drilling\n",
        "    neighbors_before_date = data.loc[neighbors_indices].loc[data['Date Fracd'] <= current_date]\n",
        "\n",
        "    # Apply formation weighting for well count\n",
        "    well_count = 0\n",
        "    cum_prod = 0\n",
        "    for neighbor_index in neighbors_before_date.index:\n",
        "        if neighbor_index != i:  # Exclude the current well\n",
        "            if neighbors_before_date.loc[neighbor_index, 'Formation'] == data.loc[i, 'Formation']:\n",
        "                well_count += 1\n",
        "            elif neighbors_before_date.loc[neighbor_index, 'Formation'] == 'MBH/TFH':\n",
        "                well_count += 0.5\n",
        "            else:\n",
        "                well_count += 0.25\n",
        "    for neighbor_index in neighbors_before_date.index:\n",
        "        if neighbor_index != i:  # Exclude the current well\n",
        "            if neighbors_before_date.loc[neighbor_index, 'Formation'] == data.loc[i, 'Formation']:\n",
        "                well_count += 1\n",
        "            elif neighbors_before_date.loc[neighbor_index, 'Formation'] == 'MBH/TFH':\n",
        "                well_count += 0.5\n",
        "            else:\n",
        "                well_count += 0.25\n",
        "\n",
        "    data.loc[i, 'Neighbors_Count_UpToDate'] = well_count\n",
        "\n",
        "#saves to csv, the process can be quite lengthy, this is to save progress\n",
        "data.to_csv('UT_Comp_With_Neighbor_Wells_1000_Formation_Weighted_Count.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e60hJmq8XkSa"
      },
      "source": [
        "#4. Bootstrapping Catboost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDyEbV-7XjfZ"
      },
      "outputs": [],
      "source": [
        "# Loading data from cell 3\n",
        "data = pd.read_csv('UT_Comp_With_Neighbor_Wells_1000_Formation_Weighted_Count.csv')\n",
        "\n",
        "# Creating ratios\n",
        "data['Lateral Length/Stages'] = data['Lateral Length']/data['Stages']\n",
        "data['Proppant/Stages'] = data['Total Prop, lbs']/data['Stages']\n",
        "\n",
        "#Drop Features\n",
        "data = data.drop(columns=['Fluid Type from DI', 'Well Name', 'Best1 Mo BOPD', 'Best9 Mo BOPD' ,'Best3 Mo BOPD',\\\n",
        "                            'Best6 Mo BOPD', 'Best12 Mo BOPD', 'Range', 'Township ', \\\n",
        "                            'SPACING_CAPPED', 'Lateral Length', 'Section', 'Unnamed: 0'])\n",
        "data = data.dropna()\n",
        "print(data.columns)\n",
        "\n",
        "#Turn Date Fracd into Year Fracd\n",
        "data['Date Fracd'] = data['Date Fracd'].str[-4:]\n",
        "data['Date Fracd'] = pd.to_numeric(data['Date Fracd'])\n",
        "\n",
        "# Get a list of categorical columns\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    data[col] = label_encoder.fit_transform(data[col])\n",
        "num_cols = data.shape[1]\n",
        "\n",
        "# Prepare train-test split\n",
        "X = data.drop(columns=['12 month Cum Prod'])\n",
        "y = data['12 month Cum Prod']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Number of bootstrap iterations\n",
        "n_bootstrap = 1 #100 used for actual code, changed to 1 for demonstration\n",
        "\n",
        "# Store predictions for uncertainty estimation\n",
        "bootstrap_preds = np.zeros((n_bootstrap, len(X_test)))\n",
        "\n",
        "# Train multiple models on resampled data\n",
        "for i in range(n_bootstrap):\n",
        "    X_resampled, y_resampled = resample(X_train, y_train, random_state=i)\n",
        "    best_param = {'learning_rate': 0.22, 'l2_leaf_reg': 7, 'iterations': 450, 'depth': 9, 'border_count': 270}\n",
        "    model = CatBoostRegressor(loss_function=\"RMSE\", silent=True, **best_param)\n",
        "    model.fit(X_resampled, y_resampled)\n",
        "\n",
        "    bootstrap_preds[i] = model.predict(X_test)\n",
        "\n",
        "\n",
        "column_names = [f'Realization {i+1}' for i in range(n_bootstrap)]\n",
        "df_pred_intervals = pd.DataFrame(bootstrap_preds.T, columns=column_names)\n",
        "\n",
        "\n",
        "# Add actual values to the DataFrame\n",
        "df_pred_intervals['Actual Value'] = y_test.values\n",
        "df_pred_intervals['X (feet)'] = data['X (feet)']\n",
        "df_pred_intervals['Y (feet)'] = data['Y (feet)']\n",
        "\n",
        "# Save results to csv\n",
        "df_pred_intervals.to_csv('prediction_intervals.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bClFn-7JYcTn"
      },
      "source": [
        "#5. Bootstrapping SHAP Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfXyUHFVYmQC"
      },
      "outputs": [],
      "source": [
        "#loading data\n",
        "data = pd.read_csv('UT_Comp_With_Neighbor_Wells_1000_Formation_Weighted_Count.csv')\n",
        "\n",
        "#zeroing x and y feet\n",
        "data['X (feet)'] = data['X (feet)'] - data['X (feet)'].min()\n",
        "data['Y (feet)'] = data['Y (feet)'] - data['Y (feet)'].min()\n",
        "\n",
        "#creating ratios\n",
        "data['Lateral Length/Stages'] = data['Lateral Length']/data['Stages']\n",
        "data['Proppant/Stages'] = data['Total Prop, lbs']/data['Stages']\n",
        "\n",
        "#Drop Features\n",
        "data = data.drop(columns=['Fluid Type from DI', 'Well Name', 'Best1 Mo BOPD', 'Best9 Mo BOPD' ,'Best3 Mo BOPD',\\\n",
        "                            'Best6 Mo BOPD', 'Best12 Mo BOPD', 'Range', 'Township ', \\\n",
        "                            'SPACING_CAPPED', 'Lateral Length', 'Section', 'Unnamed: 0'])\n",
        "data = data.dropna()\n",
        "print(data.columns)\n",
        "\n",
        "#Turn Date Fracd into Year Fracd\n",
        "data['Date Fracd'] = data['Date Fracd'].str[-4:]\n",
        "data['Date Fracd'] = pd.to_numeric(data['Date Fracd'])\n",
        "\n",
        "# Get a list of categorical columns\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    data[col] = label_encoder.fit_transform(data[col])\n",
        "num_cols = data.shape[1]\n",
        "\n",
        "# Prepare train-test split\n",
        "X = data.drop(columns=['12 month Cum Prod'])\n",
        "y = data['12 month Cum Prod']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Number of bootstrap iterations\n",
        "n_bootstrap = 100 #100 was used for actual code, 1 for demo, to not blow up nataly's computer\n",
        "\n",
        "# Store predictions for uncertainty estimation\n",
        "shap_bootstrap = np.zeros((n_bootstrap, len(X)))\n",
        "shap_bootstrap_df = pd.DataFrame(shap_bootstrap.T)\n",
        "\n",
        "# Train multiple models on resampled data\n",
        "for i in range(n_bootstrap):\n",
        "    #Leaves out 10% of the data\n",
        "    X_resampled, y_resampled = resample(X_train, y_train, random_state=i)\n",
        "    #Train the model\n",
        "    best_param = {'learning_rate': 0.22, 'l2_leaf_reg': 7, 'iterations': 450, 'depth': 9, 'border_count': 270}\n",
        "    model = CatBoostRegressor(loss_function=\"RMSE\", silent=True, **best_param)\n",
        "    model.fit(X_resampled, y_resampled)\n",
        "    #Use SHAP to explain the model\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(X)\n",
        "    shap_values_df = pd.DataFrame(shap_values, columns=[f'SHAP_{col}' for col in X.columns])\n",
        "    #Calculate the location metric\n",
        "    shap_bootstrap_df[i] = shap_values_df['SHAP_X (feet)'] + shap_values_df['SHAP_Y (feet)'] + shap_values_df['SHAP_Formation'] + shap_values_df['SHAP_Longitude'] + shap_values_df['SHAP_Latitude']\n",
        "\n",
        "\n",
        "for i in range(n_bootstrap):\n",
        "  shap_bootstrap_df = shap_bootstrap_df.rename(columns={i: f'Shap Realization {i + 1}'})\n",
        "\n",
        "# save results to csv\n",
        "shap_bootstrap_df['X (feet)'] = data['X (feet)']\n",
        "shap_bootstrap_df['Y (feet)'] = data['Y (feet)']\n",
        "shap_bootstrap_df.to_csv('shap_intervals_location.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83RGm6X9b78B"
      },
      "source": [
        "#6. Kriging Function Definition\n",
        "####a. transform_and_export\n",
        "It first randomly samples from a dataframe with 100 SHAP realizations for each well and transforms the distribution into a normal distribution\n",
        "\n",
        "####b. adopt_highest_neighbor_value\n",
        "Since the resultant distribution from the SHAP is rather noisy, we apply a smoothing algorithm to help out the kriging. We found that using highest value as opposed to simply the average outputted better models\n",
        "\n",
        "####c. grid_data\n",
        "The data was split into grids and grid blocks with values in them were averaged out and made to be the value of the grid, this is to make a lower resolution grid for the kriging algorithm later\n",
        "\n",
        "####d. kriging\n",
        "This is the main kriging function, it calls adopt_highest_neighbor_value and grid_data and then backtransforms the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w28R9ypMcLtp"
      },
      "outputs": [],
      "source": [
        "def transform_and_export(data, random_seed=None):\n",
        "    \"\"\"\n",
        "    Randomly samples from 100 columns for every row, transforms values to a normal distribution,\n",
        "    and returns a dataframe with the original sampled distribution and the transformed distribution\n",
        "\n",
        "    Args:\n",
        "        data: The input DataFrame.\n",
        "        output_file: The path to the output CSV file.\n",
        "        random_seed: The random seed to use for sampling (optional).\n",
        "\n",
        "    Returns:\n",
        "        results_df: The output dataframe with the location of each point, the randomly sampled data and the transformed value\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    # Extract columns for sampling\n",
        "    shap_columns = [f\"Shap Realization {i}\" for i in range(1, 101)]\n",
        "\n",
        "    # Create empty lists to store results\n",
        "    sampled_values = []\n",
        "    transformed_values = []\n",
        "\n",
        "    # Set random seed if provided\n",
        "    if random_seed is not None:\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    # Iterate over each row\n",
        "    for index in data.index:\n",
        "        # Randomly sample one column\n",
        "        random_column = np.random.choice(shap_columns)\n",
        "\n",
        "        # Get the value from the sampled column\n",
        "        sampled_value = data.loc[index, random_column]\n",
        "\n",
        "        # Append to the list of sampled values\n",
        "        sampled_values.append(sampled_value)\n",
        "\n",
        "    # Calculate mean and standard deviation of sampled values\n",
        "    mean = np.mean(sampled_values)\n",
        "    std = np.std(sampled_values)\n",
        "\n",
        "    # Transform sampled values to normal distribution\n",
        "    transformed_values = [(x - mean) / std for x in sampled_values]\n",
        "\n",
        "    # Create a DataFrame for the results\n",
        "    results_df = pd.DataFrame({\n",
        "        \"Sampled Value\": sampled_values,\n",
        "        \"Transformed Value\": transformed_values,\n",
        "        \"X (feet)\": data[\"X (feet)\"],\n",
        "        \"Y (feet)\": data[\"Y (feet)\"]\n",
        "        })\n",
        "\n",
        "    return results_df\n",
        "\n",
        "def adopt_highest_neighbor_value(data, distance_threshold=8000):\n",
        "    \"\"\"\n",
        "    Assigns the highest \"Transformed Value\" within a specified distance\n",
        "    to each data point. Uses cKDTree for efficient neighbor search.\n",
        "\n",
        "    Args:\n",
        "        data: The input DataFrame with \"X (feet)\", \"Y (feet)\", and \"Transformed Value\" columns.\n",
        "        distance_threshold: The maximum distance (in feet) to consider neighbors.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with the updated \"Transformed Value\" column.\n",
        "    \"\"\"\n",
        "    modified_data = data.copy()\n",
        "    coords = modified_data[[\"X (feet)\", \"Y (feet)\"]].values\n",
        "    values = modified_data[\"Transformed Value\"].values\n",
        "\n",
        "    # Build a cKDTree for efficient neighbor search\n",
        "    tree = cKDTree(coords)\n",
        "\n",
        "    for i in range(len(coords)):\n",
        "        # Find neighbors within the distance threshold\n",
        "        neighbors_indices = tree.query_ball_point(coords[i], distance_threshold)\n",
        "\n",
        "        # Exclude the current point from neighbors\n",
        "        neighbors_indices = [j for j in neighbors_indices if j != i]\n",
        "\n",
        "        # Get values of neighbors\n",
        "        neighbor_values = values[neighbors_indices]\n",
        "\n",
        "        # Assign the highest value (handle case with no neighbors)\n",
        "        if len(neighbor_values) > 0:\n",
        "            highest_value = np.max(neighbor_values)\n",
        "        else:\n",
        "            highest_value = values[i]  # Keep original value if no neighbors\n",
        "\n",
        "        # Update the \"Transformed Value\" for the current point\n",
        "        modified_data.loc[i, \"Transformed Value\"] = highest_value\n",
        "\n",
        "    return modified_data\n",
        "\n",
        "def grid_data(data, grid_size):\n",
        "    \"\"\"\n",
        "    Transforms data into a grid, averaging values within each grid cell.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get data bounds and define grid cell size\n",
        "    xmin, xmax = data[\"X (feet)\"].min(), data[\"X (feet)\"].max()\n",
        "    ymin, ymax = data[\"Y (feet)\"].min(), data[\"Y (feet)\"].max()\n",
        "    cell_size_x = (xmax - xmin) / grid_size\n",
        "    cell_size_y = (ymax - ymin) / grid_size\n",
        "\n",
        "    # Create grid centers\n",
        "    x_centers = np.linspace(xmin + cell_size_x / 2, xmax - cell_size_x / 2, grid_size)\n",
        "    y_centers = np.linspace(ymin + cell_size_y / 2, ymax - cell_size_y / 2, grid_size)\n",
        "\n",
        "    # Create meshgrid\n",
        "    grid_x_coords, grid_y_coords = np.meshgrid(x_centers, y_centers)\n",
        "\n",
        "    # Create empty grid to store averaged values\n",
        "    grid = np.full((grid_size, grid_size), np.nan)  # Initialize with NaN\n",
        "\n",
        "    # Assign averaged values to grid cells\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            # Find data points within the current grid cell\n",
        "            cell_mask = (\n",
        "                (data[\"X (feet)\"] >= x_centers[i] - cell_size_x / 2)\n",
        "                & (data[\"X (feet)\"] < x_centers[i] + cell_size_x / 2)\n",
        "                & (data[\"Y (feet)\"] >= y_centers[j] - cell_size_y / 2)\n",
        "                & (data[\"Y (feet)\"] < y_centers[j] + cell_size_y / 2)\n",
        "            )\n",
        "\n",
        "            # Calculate average value for the cell (if any data points are found)\n",
        "            if cell_mask.any():\n",
        "                grid[j, i] = data.loc[cell_mask, \"Transformed Value\"].mean()\n",
        "\n",
        "    return grid, x_centers, y_centers, grid_x_coords, grid_y_coords\n",
        "\n",
        "def kriging(data, grid_size):\n",
        "    \"\"\"Performs Ordinary Kriging and back-transforms the results.\"\"\"\n",
        "\n",
        "    original_mean = data[\"Sampled Value\"].mean()\n",
        "    original_std = data[\"Sampled Value\"].std()\n",
        "    data = adopt_highest_neighbor_value(data)\n",
        "    grid, x_centers, y_centers, grid_x_coords, grid_y_coords = grid_data(data, grid_size)\n",
        "\n",
        "    # Extract valid grid points\n",
        "    valid_mask = ~np.isnan(grid)\n",
        "    x_known = grid_x_coords[valid_mask]\n",
        "    y_known = grid_y_coords[valid_mask]\n",
        "    z_known = grid[valid_mask]\n",
        "\n",
        "    # Perform Ordinary Kriging\n",
        "    OK = OrdinaryKriging(x_known, y_known, z_known, variogram_model=\"spherical\", verbose=False, enable_plotting=False)\n",
        "    z_interp, ss = OK.execute(\"grid\", x_centers, y_centers)\n",
        "    ss = ss*original_std + original_mean\n",
        "\n",
        "    # Back-transform and return\n",
        "    z_interp_back = z_interp * original_std + original_mean\n",
        "    return x_centers, y_centers, z_interp_back, x_known, y_known, z_known, ss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNITGXeacffB"
      },
      "source": [
        "#7. Main Kriging Loop\n",
        "creates multiple kriging realizations and displays the average kriging value, the average variance, and the interval widths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1nCsax0cevX"
      },
      "outputs": [],
      "source": [
        "# Load the data from cell 5\n",
        "data = pd.read_csv('shap_intervals_location.csv')\n",
        "num_realizations = 50 #number of kriging\n",
        "# Store kriging results, known data points, and variance for averaging\n",
        "all_z_interp_back = []\n",
        "x_known = []  # Store x_known only once\n",
        "y_known = []  # Store y_known only once\n",
        "all_z_known_back = []\n",
        "all_ss = []  # Store kriging variance (ss)\n",
        "all_kriged_results = []  # Store kriged results for each realization\n",
        "\n",
        "#Main loop\n",
        "for i in range(num_realizations):\n",
        "    sampled_data = transform_and_export(data, random_seed=i)\n",
        "    x_centers, y_centers, z_interp_back, x_known_current, y_known_current, z_known, ss = kriging(sampled_data, grid_size=150)\n",
        "\n",
        "    if i == 0:  # Store x_known and y_known only once\n",
        "        x_known = x_known_current\n",
        "        y_known = y_known_current\n",
        "\n",
        "    all_z_interp_back.append(z_interp_back)\n",
        "    all_z_known_back.append(z_known)\n",
        "    all_ss.append(ss)\n",
        "\n",
        "    # Store kriged results for this realization\n",
        "    kriged_results = pd.DataFrame({\n",
        "        \"X (feet)\": np.tile(x_centers, len(y_centers)),  # Repeat x_centers for each y_center\n",
        "        \"Y (feet)\": np.repeat(y_centers, len(x_centers)),  # Repeat y_centers for each x_center\n",
        "        \"Kriged Value\": z_interp_back.flatten()  # Flatten the kriged values\n",
        "    })\n",
        "    all_kriged_results.append(kriged_results)\n",
        "\n",
        "# Calculate averages\n",
        "average_z_interp_back = np.mean(all_z_interp_back, axis=0)\n",
        "average_z_known_back = np.mean(all_z_known_back, axis=0)\n",
        "average_ss = np.mean(all_ss, axis=0)\n",
        "\n",
        "# --- Plotting ---\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.contourf(x_centers, y_centers, average_z_interp_back, levels=500, cmap=\"inferno\")\n",
        "plt.colorbar(label=\"Average Back-Transformed Kriged Value\")\n",
        "\n",
        "# Scatter plot of average known data points\n",
        "plt.scatter(x_known, y_known, c=average_z_known_back,\n",
        "            edgecolors=\"black\", label=\"Average Original Grid Cells\", cmap=\"inferno\")\n",
        "\n",
        "plt.title(\"Average Kriging Interpolation (Back-Transformed to Original Scale)\")\n",
        "plt.xlabel(\"X (feet)\")\n",
        "plt.ylabel(\"Y (feet)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Plot Kriging Variance\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.contourf(x_centers, y_centers, average_ss, levels=500, cmap=\"viridis\")  # Use a different colormap\n",
        "plt.colorbar(label=\"Average Kriging Variance\")\n",
        "plt.title(\"Kriging Variance\")\n",
        "plt.xlabel(\"X (feet)\")\n",
        "plt.ylabel(\"Y (feet)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "all_kriged_results_df = pd.concat(all_kriged_results)\n",
        "\n",
        "# Calculate interval widths for each location\n",
        "location_stats = all_kriged_results_df.groupby([\"X (feet)\", \"Y (feet)\"])[\"Kriged Value\"].agg([\"min\", \"max\"])\n",
        "location_stats[\"Interval Width\"] = location_stats[\"max\"] - location_stats[\"min\"]\n",
        "\n",
        "# Reshape interval width for plotting\n",
        "interval_width_grid = location_stats[\"Interval Width\"].values.reshape(len(y_centers), len(x_centers))\n",
        "\n",
        "# === Plot the interval width grid ===\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.contourf(x_centers, y_centers, interval_width_grid, levels=500, cmap=\"viridis\")\n",
        "plt.colorbar(label=\"Interval Width\")\n",
        "plt.title(\"Kriging Interval Width\")\n",
        "plt.xlabel(\"X (feet)\")\n",
        "plt.ylabel(\"Y (feet)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
